{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Household ID</th> <th>Individual ID</th> <th>Day of Birth</th> <th>Month of Birth</th> <th>Birth Year</th> <th>Age in Years</th> <th>Sex</th> <th>Height</th> <th>Weight</th> <th>Relationship to HH Head</th> <th>Mother ID</th> <th>Father ID</th> <th>Currently Enrolled in School</th> <th>Years of Schooling - Level</th> <th>Years of Schooling - Years</th> <th>Master ID</th> <th>Day of Interview</th> <th>Month of Interview</th> <th>Year of Interview</th> <th>Days Old</th> <th>Years Old</th> <th>Months Old</th> <th>z_scores</th> <th>Rounded Months</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>20111642    </td> <td>3            </td> <td>3           </td> <td>7             </td> <td>1976      </td> <td>31          </td> <td>2   </td> <td>1     </td> <td>-99   </td> <td>9                      </td> <td>-99      </td> <td>-99      </td> <td>-99                         </td> <td>2                         </td> <td>5                         </td> <td>-1363194477</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>11329.8 </td> <td>31.0404  </td> <td>59        </td> <td>-22.86  </td> <td>59            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>20111642    </td> <td>4            </td> <td>10          </td> <td>9             </td> <td>1995      </td> <td>11          </td> <td>2   </td> <td>1     </td> <td>-99   </td> <td>5                      </td> <td>3        </td> <td>2        </td> <td>1                           </td> <td>-99                       </td> <td>-99                       </td> <td>-1363194476</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>4322    </td> <td>11.8411  </td> <td>59        </td> <td>-22.86  </td> <td>59            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>20111642    </td> <td>5            </td> <td>20          </td> <td>4             </td> <td>1997      </td> <td>10          </td> <td>1   </td> <td>1     </td> <td>-99   </td> <td>5                      </td> <td>3        </td> <td>2        </td> <td>1                           </td> <td>-99                       </td> <td>-99                       </td> <td>-1363194475</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>3734    </td> <td>10.2301  </td> <td>59        </td> <td>-23.58  </td> <td>59            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>20111642    </td> <td>6            </td> <td>26          </td> <td>12            </td> <td>1999      </td> <td>7           </td> <td>1   </td> <td>1     </td> <td>-99   </td> <td>5                      </td> <td>3        </td> <td>2        </td> <td>-99                         </td> <td>-99                       </td> <td>-99                       </td> <td>-1363194474</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>2753.5  </td> <td>7.54384  </td> <td>59        </td> <td>-23.58  </td> <td>59            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>20111642    </td> <td>7            </td> <td>17          </td> <td>1             </td> <td>2000      </td> <td>7           </td> <td>1   </td> <td>1     </td> <td>-99   </td> <td>5                      </td> <td>3        </td> <td>2        </td> <td>-99                         </td> <td>-99                       </td> <td>-99                       </td> <td>-1363194473</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>2732.75 </td> <td>7.48699  </td> <td>59        </td> <td>-23.58  </td> <td>59            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>20111642    </td> <td>8            </td> <td>26          </td> <td>5             </td> <td>2002      </td> <td>5           </td> <td>1   </td> <td>105.9 </td> <td>22.6  </td> <td>5                      </td> <td>3        </td> <td>2        </td> <td>-99                         </td> <td>-99                       </td> <td>-99                       </td> <td>-1363194472</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>1871.25 </td> <td>5.12671  </td> <td>59        </td> <td>-0.76   </td> <td>59            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>20111642    </td> <td>9            </td> <td>5           </td> <td>1             </td> <td>2005      </td> <td>2           </td> <td>2   </td> <td>80.5  </td> <td>9.5   </td> <td>5                      </td> <td>3        </td> <td>2        </td> <td>-99                         </td> <td>-99                       </td> <td>-99                       </td> <td>-1363194471</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>918.5   </td> <td>2.51644  </td> <td>30.1148   </td> <td>-2.88   </td> <td>30            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>20111642    </td> <td>10           </td> <td>12          </td> <td>4             </td> <td>2006      </td> <td>1           </td> <td>2   </td> <td>70.3  </td> <td>7.1   </td> <td>5                      </td> <td>3        </td> <td>2        </td> <td>-99                         </td> <td>-99                       </td> <td>-99                       </td> <td>-1363194470</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>454.75  </td> <td>1.24589  </td> <td>14.9098   </td> <td>-2.27   </td> <td>15            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>20113642    </td> <td>1            </td> <td>13          </td> <td>4             </td> <td>1966      </td> <td>41          </td> <td>1   </td> <td>1     </td> <td>-99   </td> <td>1                      </td> <td>-99      </td> <td>-99      </td> <td>-99                         </td> <td>-99                       </td> <td>-99                       </td> <td>-1361194479</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>15063.8 </td> <td>41.2705  </td> <td>59        </td> <td>-23.58  </td> <td>59            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>20113642    </td> <td>2            </td> <td>1           </td> <td>1             </td> <td>1969      </td> <td>38          </td> <td>2   </td> <td>1     </td> <td>-99   </td> <td>2                      </td> <td>-99      </td> <td>-99      </td> <td>-99                         </td> <td>-99                       </td> <td>-99                       </td> <td>-1361194478</td> <td>10              </td> <td>7                 </td> <td>2007             </td> <td>14071.5 </td> <td>38.5521  </td> <td>59        </td> <td>-22.86  </td> <td>59            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (23002 rows omitted)</p"
      ],
      "text/plain": [
       "Household ID | Individual ID | Day of Birth | Month of Birth | Birth Year | Age in Years | Sex  | Height | Weight | Relationship to HH Head | Mother ID | Father ID | Currently Enrolled in School | Years of Schooling - Level | Years of Schooling - Years | Master ID   | Day of Interview | Month of Interview | Year of Interview | Days Old | Years Old | Months Old | z_scores | Rounded Months\n",
       "20111642     | 3             | 3            | 7              | 1976       | 31           | 2    | 1      | -99    | 9                       | -99       | -99       | -99                          | 2                          | 5                          | -1363194477 | 10               | 7                  | 2007              | 11329.8  | 31.0404   | 59         | -22.86   | 59\n",
       "20111642     | 4             | 10           | 9              | 1995       | 11           | 2    | 1      | -99    | 5                       | 3         | 2         | 1                            | -99                        | -99                        | -1363194476 | 10               | 7                  | 2007              | 4322     | 11.8411   | 59         | -22.86   | 59\n",
       "20111642     | 5             | 20           | 4              | 1997       | 10           | 1    | 1      | -99    | 5                       | 3         | 2         | 1                            | -99                        | -99                        | -1363194475 | 10               | 7                  | 2007              | 3734     | 10.2301   | 59         | -23.58   | 59\n",
       "20111642     | 6             | 26           | 12             | 1999       | 7            | 1    | 1      | -99    | 5                       | 3         | 2         | -99                          | -99                        | -99                        | -1363194474 | 10               | 7                  | 2007              | 2753.5   | 7.54384   | 59         | -23.58   | 59\n",
       "20111642     | 7             | 17           | 1              | 2000       | 7            | 1    | 1      | -99    | 5                       | 3         | 2         | -99                          | -99                        | -99                        | -1363194473 | 10               | 7                  | 2007              | 2732.75  | 7.48699   | 59         | -23.58   | 59\n",
       "20111642     | 8             | 26           | 5              | 2002       | 5            | 1    | 105.9  | 22.6   | 5                       | 3         | 2         | -99                          | -99                        | -99                        | -1363194472 | 10               | 7                  | 2007              | 1871.25  | 5.12671   | 59         | -0.76    | 59\n",
       "20111642     | 9             | 5            | 1              | 2005       | 2            | 2    | 80.5   | 9.5    | 5                       | 3         | 2         | -99                          | -99                        | -99                        | -1363194471 | 10               | 7                  | 2007              | 918.5    | 2.51644   | 30.1148    | -2.88    | 30\n",
       "20111642     | 10            | 12           | 4              | 2006       | 1            | 2    | 70.3   | 7.1    | 5                       | 3         | 2         | -99                          | -99                        | -99                        | -1363194470 | 10               | 7                  | 2007              | 454.75   | 1.24589   | 14.9098    | -2.27    | 15\n",
       "20113642     | 1             | 13           | 4              | 1966       | 41           | 1    | 1      | -99    | 1                       | -99       | -99       | -99                          | -99                        | -99                        | -1361194479 | 10               | 7                  | 2007              | 15063.8  | 41.2705   | 59         | -23.58   | 59\n",
       "20113642     | 2             | 1            | 1              | 1969       | 38           | 2    | 1      | -99    | 2                       | -99       | -99       | -99                          | -99                        | -99                        | -1361194478 | 10               | 7                  | 2007              | 14071.5  | 38.5521   | 59         | -22.86   | 59\n",
       "... (23002 rows omitted)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datascience import *\n",
    "from pandas import read_stata\n",
    "import numpy as np\n",
    "from pygrowup import Calculator\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg', warn=False)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "data = Table.read_table('Lab_3.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Modeling\n",
    "Throughout the semester, we have cleaned and explored data on a variety of categories.  Today is all about combining this data to find a model for child HAZ (for kids under 5) or years of schooling (for kids over 5) that best fit your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Blue\"> Item 1: Make a diagram that illustrates the relationship between the variables and the outcome (Schooling or HAZ).  This can be your own simplified interpretation.  You can use some ideas from the end of the education chapter.  This is not a data exercise, but a thought exercise of considering theoretical pathways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test your theory.  Select the 3 most important variables.\n",
    "<font color=\"Blue\"> Item 2: Randomly split the data into 80% & 20%.  With the training set (80%), build a k-nearest neighbors classifier.  You can pick your k, but may need to adjust this number based on your data set.  With this classifier, test it on your test set (20%).  What is the accuracy of your model?  Also, what is the false positive rate?  Use a table to present these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2870.4, 3588)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.num_rows * .8, data.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled = data.sample(with_replacement=False)\n",
    "training = shuffled.take(np.arange(***number here***))\n",
    "testing = shuffled.take(np.arange(***number here***, ***number here***))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure your data has a column with your desired outcome with true/false values.  \n",
    "\n",
    "training['above average'] = training['z_scores'] >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Household ID</th> <th>Individual ID</th> <th>Day of Birth</th> <th>Month of Birth</th> <th>Birth Year</th> <th>Age</th> <th>Sex</th> <th>height</th> <th>weight</th> <th>Relationship to HH Head</th> <th>Mother ID</th> <th>Father ID</th> <th>Education Level</th> <th>Edu Class</th> <th>Master ID</th> <th>Annualized sum</th> <th>Day of Interview</th> <th>Month of Interview</th> <th>Year of Interview</th> <th>Days Old</th> <th>Years Old</th> <th>Months Old</th> <th>z_scores</th> <th>Rounded Months</th> <th>above average</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>35535326    </td> <td>7            </td> <td>7           </td> <td>8             </td> <td>2002      </td> <td>4   </td> <td>2   </td> <td>98.4  </td> <td>13.5  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>35535326007</td> <td>-99           </td> <td>5               </td> <td>6                 </td> <td>2007             </td> <td>1763.25 </td> <td>4.83082  </td> <td>57.9699   </td> <td>-2.03   </td> <td>58            </td> <td>False        </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>35225310    </td> <td>3            </td> <td>1           </td> <td>9             </td> <td>2006      </td> <td>0   </td> <td>2   </td> <td>57.2  </td> <td>6.2   </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>35225310003</td> <td>-99           </td> <td>13              </td> <td>2                 </td> <td>2007             </td> <td>163.75  </td> <td>0.44863  </td> <td>5.38356   </td> <td>-3.08   </td> <td>5             </td> <td>False        </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>22528537    </td> <td>9            </td> <td>16          </td> <td>7             </td> <td>2007      </td> <td>0   </td> <td>2   </td> <td>48.2  </td> <td>4     </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>22528537009</td> <td>-99           </td> <td>7               </td> <td>8                 </td> <td>2007             </td> <td>21.5    </td> <td>0.0589041</td> <td>0.706849  </td> <td>-2.22   </td> <td>1             </td> <td>False        </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>35415342    </td> <td>9            </td> <td>5           </td> <td>11            </td> <td>2005      </td> <td>2   </td> <td>2   </td> <td>76.1  </td> <td>8.3   </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>35415342009</td> <td>-99           </td> <td>20              </td> <td>11                </td> <td>2007             </td> <td>745.5   </td> <td>2.04247  </td> <td>24.5096   </td> <td>-2.98   </td> <td>25            </td> <td>False        </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>47624126    </td> <td>6            </td> <td>12          </td> <td>8             </td> <td>2004      </td> <td>2   </td> <td>1   </td> <td>84.3  </td> <td>11.1  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>47624126006</td> <td>-99           </td> <td>12              </td> <td>6                 </td> <td>2007             </td> <td>1034.75 </td> <td>2.83493  </td> <td>34.0192   </td> <td>-2.89   </td> <td>34            </td> <td>False        </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>45824154    </td> <td>5            </td> <td>2           </td> <td>2             </td> <td>2006      </td> <td>1   </td> <td>2   </td> <td>82.7  </td> <td>10    </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>45824154005</td> <td>-99           </td> <td>22              </td> <td>1                 </td> <td>2008             </td> <td>720     </td> <td>1.9726   </td> <td>23.6712   </td> <td>-0.89   </td> <td>24            </td> <td>False        </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>25525218    </td> <td>5            </td> <td>18          </td> <td>4             </td> <td>2003      </td> <td>3   </td> <td>1   </td> <td>91.1  </td> <td>11.6  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>25525218005</td> <td>-99           </td> <td>17              </td> <td>4                 </td> <td>2007             </td> <td>1460    </td> <td>4        </td> <td>48        </td> <td>-2.92   </td> <td>48            </td> <td>False        </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>42116554    </td> <td>8            </td> <td>24          </td> <td>10            </td> <td>2005      </td> <td>2   </td> <td>2   </td> <td>80.5  </td> <td>10.2  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>42116554008</td> <td>-99           </td> <td>8               </td> <td>1                 </td> <td>2008             </td> <td>805.25  </td> <td>2.20616  </td> <td>26.474    </td> <td>-2.09   </td> <td>26            </td> <td>False        </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>37334324    </td> <td>6            </td> <td>19          </td> <td>6             </td> <td>2003      </td> <td>3   </td> <td>2   </td> <td>88.6  </td> <td>13.4  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>37334324006</td> <td>-99           </td> <td>22              </td> <td>5                 </td> <td>2007             </td> <td>1433.5  </td> <td>3.9274   </td> <td>47.1288   </td> <td>-3.17   </td> <td>47            </td> <td>False        </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>28813213    </td> <td>7            </td> <td>26          </td> <td>5             </td> <td>2003      </td> <td>3   </td> <td>2   </td> <td>106.4 </td> <td>17.3  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>28813213007</td> <td>-99           </td> <td>6               </td> <td>3                 </td> <td>2007             </td> <td>1380    </td> <td>3.78082  </td> <td>45.3699   </td> <td>1.31    </td> <td>45            </td> <td>True         </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (2861 rows omitted)</p"
      ],
      "text/plain": [
       "Household ID | Individual ID | Day of Birth | Month of Birth | Birth Year | Age  | Sex  | height | weight | Relationship to HH Head | Mother ID | Father ID | Education Level | Edu Class | Master ID   | Annualized sum | Day of Interview | Month of Interview | Year of Interview | Days Old | Years Old | Months Old | z_scores | Rounded Months | above average\n",
       "35535326     | 7             | 7            | 8              | 2002       | 4    | 2    | 98.4   | 13.5   | 3                       | 2         | 1         | -99             | -99       | 35535326007 | -99            | 5                | 6                  | 2007              | 1763.25  | 4.83082   | 57.9699    | -2.03    | 58             | False\n",
       "35225310     | 3             | 1            | 9              | 2006       | 0    | 2    | 57.2   | 6.2    | 3                       | 2         | 1         | -99             | -99       | 35225310003 | -99            | 13               | 2                  | 2007              | 163.75   | 0.44863   | 5.38356    | -3.08    | 5              | False\n",
       "22528537     | 9             | 16           | 7              | 2007       | 0    | 2    | 48.2   | 4      | 3                       | 2         | 1         | -99             | -99       | 22528537009 | -99            | 7                | 8                  | 2007              | 21.5     | 0.0589041 | 0.706849   | -2.22    | 1              | False\n",
       "35415342     | 9             | 5            | 11             | 2005       | 2    | 2    | 76.1   | 8.3    | 3                       | 2         | 1         | -99             | -99       | 35415342009 | -99            | 20               | 11                 | 2007              | 745.5    | 2.04247   | 24.5096    | -2.98    | 25             | False\n",
       "47624126     | 6             | 12           | 8              | 2004       | 2    | 1    | 84.3   | 11.1   | 3                       | 2         | 1         | -99             | -99       | 47624126006 | -99            | 12               | 6                  | 2007              | 1034.75  | 2.83493   | 34.0192    | -2.89    | 34             | False\n",
       "45824154     | 5             | 2            | 2              | 2006       | 1    | 2    | 82.7   | 10     | 3                       | 2         | 1         | -99             | -99       | 45824154005 | -99            | 22               | 1                  | 2008              | 720      | 1.9726    | 23.6712    | -0.89    | 24             | False\n",
       "25525218     | 5             | 18           | 4              | 2003       | 3    | 1    | 91.1   | 11.6   | 3                       | 2         | 1         | -99             | -99       | 25525218005 | -99            | 17               | 4                  | 2007              | 1460     | 4         | 48         | -2.92    | 48             | False\n",
       "42116554     | 8             | 24           | 10             | 2005       | 2    | 2    | 80.5   | 10.2   | 3                       | 2         | 1         | -99             | -99       | 42116554008 | -99            | 8                | 1                  | 2008              | 805.25   | 2.20616   | 26.474     | -2.09    | 26             | False\n",
       "37334324     | 6             | 19           | 6              | 2003       | 3    | 2    | 88.6   | 13.4   | 3                       | 2         | 1         | -99             | -99       | 37334324006 | -99            | 22               | 5                  | 2007              | 1433.5   | 3.9274    | 47.1288    | -3.17    | 47             | False\n",
       "28813213     | 7             | 26           | 5              | 2003       | 3    | 2    | 106.4  | 17.3   | 3                       | 2         | 1         | -99             | -99       | 28813213007 | -99            | 6                | 3                  | 2007              | 1380     | 3.78082   | 45.3699    | 1.31     | 45             | True\n",
       "... (2861 rows omitted)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kNN_3_dimensions(table, three_columns, classifier_column, point, k):\n",
    "    \"\"\"\n",
    "    TABLE is your training set\n",
    "    THREE_COLUMNS is a list of the three columns that you will get your x, y, and z coordinates from\n",
    "    CLASSIFIER_COLUMN is the column where we hold the class of our data\n",
    "    POINT is the point that we are computing the distance to\n",
    "    K is the k in kNN\"\"\"\n",
    "    \n",
    "    def euclidean_distance_3_dimensions(data1, data2):\n",
    "        \"\"\"Returns the distance between two points that have three dimensions in the form of a lists\"\"\"\n",
    "        return ((data1[0] - data2[0]) ** (2) + (data1[1] - data2[1]) ** (2) + (data1[2] - data2[2]) ** (2)) ** (1/2)\n",
    "\n",
    "    # pull out the arrays of our coordinates\n",
    "    xs = table[three_columns[0]]\n",
    "    ys = table[three_columns[1]]\n",
    "    zs = table[three_columns[2]]\n",
    "    classified_as = table[classifier_column]\n",
    "    \n",
    "    # creating a table and appending our computed distances and their classifier\n",
    "    computed_distances = Table(['Distance','Class'])\n",
    "    for i in range(len(table)):\n",
    "        x = xs[i]\n",
    "        y = ys[i]\n",
    "        z = zs[i]\n",
    "        compared_point = [x, y, z]\n",
    "        distance = euclidean_distance_3_dimensions(compared_point, point)\n",
    "        computed_distances.append([distance, classified_as[i]])\n",
    "        \n",
    "    def sort_distances_return_k_closest(table, sort_on, K):\n",
    "        return table.sort(sort_on).take(np.arange(K))\n",
    "    \n",
    "    def most_in_k(table, classify_on):\n",
    "        return table.group(classify_on).sort('count', descending = True).column(classify_on).item(0)\n",
    "    \n",
    "    sorted_table = sort_distances_return_k_closest(computed_distances, \"Distance\", k)\n",
    "    return most_in_k(sorted_table, \"Class\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Household ID</th> <th>Individual ID</th> <th>Day of Birth</th> <th>Month of Birth</th> <th>Birth Year</th> <th>Age</th> <th>Sex</th> <th>height</th> <th>weight</th> <th>Relationship to HH Head</th> <th>Mother ID</th> <th>Father ID</th> <th>Education Level</th> <th>Edu Class</th> <th>Master ID</th> <th>Annualized sum</th> <th>Day of Interview</th> <th>Month of Interview</th> <th>Year of Interview</th> <th>Days Old</th> <th>Years Old</th> <th>Months Old</th> <th>z_scores</th> <th>Rounded Months</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>22112547    </td> <td>7            </td> <td>13          </td> <td>3             </td> <td>2007      </td> <td>0   </td> <td>2   </td> <td>60.1  </td> <td>7.5   </td> <td>5                      </td> <td>4        </td> <td>3        </td> <td>-99            </td> <td>-99      </td> <td>22112547007</td> <td>-99           </td> <td>16              </td> <td>10                </td> <td>2007             </td> <td>216.5   </td> <td>0.593151 </td> <td>7.11781   </td> <td>-3.1    </td> <td>7             </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>30623447    </td> <td>5            </td> <td>14          </td> <td>7             </td> <td>2004      </td> <td>2   </td> <td>1   </td> <td>94.5  </td> <td>12    </td> <td>5                      </td> <td>-99      </td> <td>-99      </td> <td>-99            </td> <td>-99      </td> <td>30623447005</td> <td>-99           </td> <td>13              </td> <td>3                 </td> <td>2007             </td> <td>972.75  </td> <td>2.66507  </td> <td>31.9808   </td> <td>0.53    </td> <td>32            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>30327424    </td> <td>15           </td> <td>14          </td> <td>1             </td> <td>2004      </td> <td>3   </td> <td>1   </td> <td>91.5  </td> <td>12.7  </td> <td>5                      </td> <td>4        </td> <td>12       </td> <td>-99            </td> <td>-99      </td> <td>30327424015</td> <td>-99           </td> <td>24              </td> <td>4                 </td> <td>2007             </td> <td>1197.25 </td> <td>3.28014  </td> <td>39.3616   </td> <td>-1.69   </td> <td>39            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>30423407    </td> <td>9            </td> <td>1           </td> <td>8             </td> <td>2003      </td> <td>3   </td> <td>1   </td> <td>86    </td> <td>10.4  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>30423407009</td> <td>-99           </td> <td>16              </td> <td>1                 </td> <td>2007             </td> <td>1262.5  </td> <td>3.4589   </td> <td>41.5068   </td> <td>-3.38   </td> <td>42            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>27333252    </td> <td>4            </td> <td>20          </td> <td>3             </td> <td>2003      </td> <td>4   </td> <td>2   </td> <td>103.7 </td> <td>12.4  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>27333252004</td> <td>-99           </td> <td>8               </td> <td>1                 </td> <td>2008             </td> <td>1753.25 </td> <td>4.80342  </td> <td>57.6411   </td> <td>-0.89   </td> <td>58            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>47313120    </td> <td>6            </td> <td>4           </td> <td>5             </td> <td>2005      </td> <td>2   </td> <td>2   </td> <td>80.2  </td> <td>8.9   </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>47313120006</td> <td>-99           </td> <td>8               </td> <td>5                 </td> <td>2007             </td> <td>734.5   </td> <td>2.01233  </td> <td>24.1479   </td> <td>-1.71   </td> <td>24            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>22433542    </td> <td>5            </td> <td>10          </td> <td>8             </td> <td>2005      </td> <td>2   </td> <td>1   </td> <td>80.3  </td> <td>11.5  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>22433542005</td> <td>-99           </td> <td>2               </td> <td>10                </td> <td>2007             </td> <td>783.5   </td> <td>2.14658  </td> <td>25.7589   </td> <td>-2.46   </td> <td>26            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>36613318    </td> <td>3            </td> <td>3           </td> <td>4             </td> <td>2005      </td> <td>2   </td> <td>1   </td> <td>87.2  </td> <td>10.5  </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>36613318003</td> <td>-99           </td> <td>10              </td> <td>4                 </td> <td>2007             </td> <td>737.5   </td> <td>2.02055  </td> <td>24.2466   </td> <td>0.03    </td> <td>24            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>44733113    </td> <td>7            </td> <td>23          </td> <td>3             </td> <td>2006      </td> <td>1   </td> <td>1   </td> <td>68.7  </td> <td>6.9   </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>44733113007</td> <td>-99           </td> <td>3               </td> <td>4                 </td> <td>2007             </td> <td>375.75  </td> <td>1.02945  </td> <td>12.3534   </td> <td>-2.97   </td> <td>12            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>48311135    </td> <td>6            </td> <td>14          </td> <td>12            </td> <td>2006      </td> <td>0   </td> <td>2   </td> <td>62    </td> <td>6.7   </td> <td>3                      </td> <td>2        </td> <td>1        </td> <td>-99            </td> <td>-99      </td> <td>48311135006</td> <td>-99           </td> <td>14              </td> <td>8                 </td> <td>2007             </td> <td>243.25  </td> <td>0.666438 </td> <td>7.99726   </td> <td>-2.28   </td> <td>8             </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (707 rows omitted)</p"
      ],
      "text/plain": [
       "Household ID | Individual ID | Day of Birth | Month of Birth | Birth Year | Age  | Sex  | height | weight | Relationship to HH Head | Mother ID | Father ID | Education Level | Edu Class | Master ID   | Annualized sum | Day of Interview | Month of Interview | Year of Interview | Days Old | Years Old | Months Old | z_scores | Rounded Months\n",
       "22112547     | 7             | 13           | 3              | 2007       | 0    | 2    | 60.1   | 7.5    | 5                       | 4         | 3         | -99             | -99       | 22112547007 | -99            | 16               | 10                 | 2007              | 216.5    | 0.593151  | 7.11781    | -3.1     | 7\n",
       "30623447     | 5             | 14           | 7              | 2004       | 2    | 1    | 94.5   | 12     | 5                       | -99       | -99       | -99             | -99       | 30623447005 | -99            | 13               | 3                  | 2007              | 972.75   | 2.66507   | 31.9808    | 0.53     | 32\n",
       "30327424     | 15            | 14           | 1              | 2004       | 3    | 1    | 91.5   | 12.7   | 5                       | 4         | 12        | -99             | -99       | 30327424015 | -99            | 24               | 4                  | 2007              | 1197.25  | 3.28014   | 39.3616    | -1.69    | 39\n",
       "30423407     | 9             | 1            | 8              | 2003       | 3    | 1    | 86     | 10.4   | 3                       | 2         | 1         | -99             | -99       | 30423407009 | -99            | 16               | 1                  | 2007              | 1262.5   | 3.4589    | 41.5068    | -3.38    | 42\n",
       "27333252     | 4             | 20           | 3              | 2003       | 4    | 2    | 103.7  | 12.4   | 3                       | 2         | 1         | -99             | -99       | 27333252004 | -99            | 8                | 1                  | 2008              | 1753.25  | 4.80342   | 57.6411    | -0.89    | 58\n",
       "47313120     | 6             | 4            | 5              | 2005       | 2    | 2    | 80.2   | 8.9    | 3                       | 2         | 1         | -99             | -99       | 47313120006 | -99            | 8                | 5                  | 2007              | 734.5    | 2.01233   | 24.1479    | -1.71    | 24\n",
       "22433542     | 5             | 10           | 8              | 2005       | 2    | 1    | 80.3   | 11.5   | 3                       | 2         | 1         | -99             | -99       | 22433542005 | -99            | 2                | 10                 | 2007              | 783.5    | 2.14658   | 25.7589    | -2.46    | 26\n",
       "36613318     | 3             | 3            | 4              | 2005       | 2    | 1    | 87.2   | 10.5   | 3                       | 2         | 1         | -99             | -99       | 36613318003 | -99            | 10               | 4                  | 2007              | 737.5    | 2.02055   | 24.2466    | 0.03     | 24\n",
       "44733113     | 7             | 23           | 3              | 2006       | 1    | 1    | 68.7   | 6.9    | 3                       | 2         | 1         | -99             | -99       | 44733113007 | -99            | 3                | 4                  | 2007              | 375.75   | 1.02945   | 12.3534    | -2.97    | 12\n",
       "48311135     | 6             | 14           | 12             | 2006       | 0    | 2    | 62     | 6.7    | 3                       | 2         | 1         | -99             | -99       | 48311135006 | -99            | 14               | 8                  | 2007              | 243.25   | 0.666438  | 7.99726    | -2.28    | 8\n",
       "... (707 rows omitted)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kNN_3_dimensions(training, ['Age', 'Sex', 'height'], 'above average', [0, 2, 60.1], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"Blue\"> Item 3: Repeat the above activity with the three variables you think are the least important.  What is the accuracy and comment on its difference from Item 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Blue\"> Item 4:  Now try different combinations of a variety of variables (not limited to three - use as many as needed) to acheive the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression\n",
    "\n",
    "<font color=\"Blue\"> Another way to predict outcomes is with multivariate regression.  Prepare two multivariate regressions to compare: one regression will have all the variables you think are important and the other will have variables you think are not important.  Compare the R^2 of these regressions.  Were you right? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Just run this cell.  You should also at least read the\n",
    "# documentation for the functions (the stuff inside the\n",
    "# triple-double-quotes that describes what each function\n",
    "# does), since you'll need to call them soon.\n",
    "\n",
    "def predict_all(features_table, coefficients):\n",
    "    \"\"\"\n",
    "    Given a table of features called features_table and some coefficients,\n",
    "    produces linear predictions for each row of features_table.\n",
    "    \n",
    "    features_table should be a table with one column for each feature\n",
    "    being used to predict.  Each row represents a house in the task\n",
    "    we're doing in this lab.\n",
    "    \n",
    "    coefficients should be an array with one element for each column in\n",
    "    features_table, like the coefficients computed by the function\n",
    "    least_squares_coefficients.\n",
    "    \n",
    "    Returns an array of predictions, one for each row of features_table.\n",
    "    \n",
    "    For example, in the house price prediction task we're working on in\n",
    "    this lab, each row of data is the features of one house sale, like\n",
    "    the number of bedrooms or the size of the house.  To make a\n",
    "    prediction for the price of a house, we multiply each of these\n",
    "    features by the corresponding coefficient in the coefficients\n",
    "    array, then sum the results.\n",
    "    \n",
    "    An even more detailed example: Suppose we have just one house whose\n",
    "    price we want to predict, and two features, Bedrooms and Size.\n",
    "    Then features_table will have 1 row and 2 columns, Bedrooms and Size.\n",
    "    Say their values are 2 and 1500.  The coefficients array will have 2\n",
    "    numbers in it, say -1000 and 200.  Our prediction for this house is:\n",
    "\n",
    "        features_table.column(0).item(0)*coefficients.item(0) + \\\n",
    "        data.column(1).item(0)*coefficients.item(1)\n",
    "    \n",
    "    or\n",
    "    \n",
    "        2*-1000 + 1500*200\n",
    "    \n",
    "    which is $298,000.  So we will return an array with just one number\n",
    "    in it, and it will look like:\n",
    "    \n",
    "        np.array([298000])\n",
    "    \n",
    "    If there were 3 rows in features_table, we would return a 3-element\n",
    "    array instead, containing the predicted prices for each row.\n",
    "    \"\"\"\n",
    "    assert features_table.num_columns == len(coefficients)\n",
    "    \"\"\"\n",
    "    The first argument to predict_all should be a table with one\n",
    "    column for each feature.  That means it should have the same\n",
    "    number of columns as the coefficients array (the second\n",
    "    argument) has elements.\n",
    "    \"\"\"\n",
    "    def predict(features):\n",
    "        # Given an array of features, produce one prediction.\n",
    "        return sum(features * coefficients)\n",
    "    predictions = Table().with_column('features', features_table.rows).apply(predict, 'features')\n",
    "    return predictions\n",
    "\n",
    "def compute_errors(features_table, coefficients, true_values):\n",
    "    \"\"\"\n",
    "    Computes the prediction errors for a linear model with the given\n",
    "    coefficients when predicting the true values for the given\n",
    "    examples.\n",
    "    \n",
    "    features_table should be a table with one column for each feature\n",
    "    being used to predict.  Each row represents a house in the task\n",
    "    we're doing in this lab.\n",
    "    \n",
    "    coefficients should be an array of numbers, one for each feature.\n",
    "    \n",
    "    true_values should be an array of numbers, one for each row in\n",
    "    features_table.  It records the true prices of each house.\n",
    "    \"\"\"\n",
    "    return predict_all(features_table, coefficients) - true_values\n",
    "\n",
    "def rmse(errors):\n",
    "    \"\"\"\n",
    "    Computes the root mean squared error when a regression model makes\n",
    "    the given errors.  So errors should be an array of numbers, one for\n",
    "    each row in some data table for which we're computing predictions\n",
    "    (that is, each house).  Each number is the prediction error of some\n",
    "    regression model (when predicting the price of a house).\n",
    "    \"\"\"\n",
    "    return np.mean(errors**2)**0.5\n",
    "\n",
    "def make_least_squares_objective_function(features_table, true_values):\n",
    "    \"\"\"\n",
    "    Makes an objective function for training data in the features_table\n",
    "    table, where the true values we're trying to predict are true_values.\n",
    "    \n",
    "    features_table should be a table with one column for each feature\n",
    "    being used to predict.  Each row represents a house in the task\n",
    "    we're doing in this lab.\n",
    "    \n",
    "    true_values should be an array of numbers, one for each row in\n",
    "    features_table.  It records the true prices of each house.\n",
    "    \n",
    "    The returned value is a function.  That function takes an array of\n",
    "    coefficients and returns a number.  Larger values of that number\n",
    "    mean that those coefficients produce worse prediction errors.\n",
    "    \"\"\"\n",
    "    def objective_function(coefficients):\n",
    "        errors = compute_errors(features_table, np.array(coefficients), true_values)\n",
    "        return rmse(errors)\n",
    "    return objective_function\n",
    "\n",
    "def least_squares_coefficients(training_data, predicted_column_name):\n",
    "    \"\"\"\n",
    "    Performs multiple linear regression predicting predicted_column_name\n",
    "    using the other columns of training_data as features.\n",
    "    \n",
    "    training_data should be a table with one column for each feature\n",
    "    being used to predict, plus one column for the value we're trying\n",
    "    to predict.  That column's name should equal predicted_column_name.\n",
    "    Each row represents a house in the task we're doing in this lab.\n",
    "    \n",
    "    predicted_column_name should be a string, the name of the column in\n",
    "    training_data that we're trying to predict.\n",
    "    \n",
    "    Returns an array of coefficients, one for each feature (that is, one\n",
    "    for each column in training_data other than predicted_column_name).\n",
    "    \n",
    "    For example, if training_data has 3 columns, Bedroom, Size, and Price,\n",
    "    and predicted_column_name is \"Price\", then we will use Bedroom and\n",
    "    Size to predict Price.  This function will return an array of 2\n",
    "    numbers, a regression coefficient for Bedroom (like -1000) and a\n",
    "    regression coefficient for Size (like 200).\n",
    "    \"\"\"\n",
    "    features_table = training_data.drop(predicted_column_name)\n",
    "    true_values = training_data.column(predicted_column_name)\n",
    "    objective_function = make_least_squares_objective_function(features_table, true_values)\n",
    "    \n",
    "    # Now we find the coefficients that produce the smallest\n",
    "    # error.\n",
    "    initial_coefficient_guess = np.zeros(features_table.num_columns)\n",
    "    best_coefficients = minimize(objective_function, start=initial_coefficient_guess)\n",
    "    if features_table.num_columns == 1:\n",
    "        return np.array([best_coefficients])\n",
    "    else:\n",
    "        return best_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"Household ID\"]=data.apply(int, 'Household ID')\n",
    "data[\"Individual ID\"]=data.apply(int, 'Individual ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-2ba3d1d97e29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjust_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Household ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Individual ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjust_size_coefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_coefficients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjust_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Individual ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-c12f2ff5e499>\u001b[0m in \u001b[0;36mleast_squares_coefficients\u001b[0;34m(training_data, predicted_column_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0minitial_coefficient_guess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mbest_coefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_coefficient_guess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_columns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_coefficients\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SarahAnneReynolds\\Anaconda3\\envs\\ds8\\lib\\site-packages\\datascience\\util.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(f, start, smooth, log, array, **vargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msmooth\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'method'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mvargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'method'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Powell'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SarahAnneReynolds\\Anaconda3\\envs\\ds8\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_neldermead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'powell'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_powell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SarahAnneReynolds\\Anaconda3\\envs\\ds8\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_powell\u001b[0;34m(func, x0, args, callback, xtol, ftol, maxiter, maxfev, disp, direc, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   2339\u001b[0m         \u001b[0mdirec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     \u001b[0mfval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2342\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m     \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SarahAnneReynolds\\Anaconda3\\envs\\ds8\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SarahAnneReynolds\\Anaconda3\\envs\\ds8\\lib\\site-packages\\datascience\\util.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msmooth\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'method'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-c12f2ff5e499>\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(coefficients)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobjective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-c12f2ff5e499>\u001b[0m in \u001b[0;36mcompute_errors\u001b[0;34m(features_table, coefficients, true_values)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mfeatures_table\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mIt\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0mprices\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mhouse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \"\"\"\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredict_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrue_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-c12f2ff5e499>\u001b[0m in \u001b[0;36mpredict_all\u001b[0;34m(features_table, coefficients)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0marray\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mprices\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mfeatures_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_columns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[1;32m     52\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0margument\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpredict_all\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "just_size = data.select([\"Household ID\", \"Individual ID\"])\n",
    "just_size_coefficients = least_squares_coefficients(just_size, \"Individual ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-92b6cd75d051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# regression line, is the one where the RMSE is smallest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mslopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coefficient on Size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mobjective_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_least_squares_objective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Price\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective_value_for_slope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslope\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Just run this cell.  It computes a bunch of slopes for potential\n",
    "# lines, then computes the root-mean-squared error for each one.\n",
    "# The best line for this dataset, a.k.a. the least-squares\n",
    "# regression line, is the one where the RMSE is smallest.\n",
    "slopes = Table().with_column(\"coefficient on Size\", np.arange(-1000, 1000, 10))\n",
    "objective_function = make_least_squares_objective_function(train.select(\"Size\"), train.column(\"Price\"))\n",
    "def objective_value_for_slope(slope):\n",
    "    return objective_function(np.array([slope]))\n",
    "errors = slopes.with_column(\"RMSE\", slopes.apply(objective_value_for_slope, \"coefficient on Size\"))\n",
    "errors.scatter(\"coefficient on Size\", \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Blue\"> Item 6: Let's be more systematic about building the model.  Adding more covariates will improve the fit of the model, but we are also making the model more complex.  Let's add covariates in order of importance, keeping those that increase the adjusted R^2.  The formula for the adjusted R^2 is 1-(1-R^2)(N-1)/(N-p-1), where N= sample size and p= number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Blue\"> Item 6: Let's be more systematic about building the model.  Adding more covariates will improve the fit of the model, but we are also making the model more complex.  Let's add covariates in order of importance, keeping those that increase the adjusted R^2.  The formula for the adjusted R^2 is 1-(1-R^2)(N-1)/(N-p-1), where N= sample size and p= number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is of correlation, not causation.  But since this is a first indication of a potential causal relationship, we may as well try a policy and then later see if it works out. <font color=\"Blue\"> Item 8: If you were attempting to change the outcome based on changing the feature, which feature would you try to change?  Don't just consider the one with the highest correlation, but also take into account costs and difficulty of changing the feature. (Not a data exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Blue\"> Item 6: Now trade coefficients with someone who is working with data from another country.  How well did their model fit your data: what is the accuracy of their model across borders? Comment on what this implies for policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Blue\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
